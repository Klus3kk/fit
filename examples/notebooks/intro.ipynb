{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT Library Showcase\n",
    "\n",
    "This notebook demonstrates the features of the FIT library - a machine learning framework built with the help of NumPy.\n",
    "\n",
    "The FIT library that I've created provides tensor operations with automatic differentiation, neural network components with inclusion of linear layers, activations, and normalization, attention mechanisms and transformers, multiple optimizers such as SGD, Adam, SAM, and Lion, loss functions for regression and classification, data pipeline utilities with built-in datasets, training monitoring and visualization, model persistence for saving and loading, and APIs for quick experimentation.\n",
    "\n",
    "I tried to make this library lightweight and it only requires NumPy. It offers a PyTorch-like familiar API, and has an extensible clean architecture for adding components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/Klus3kk/fit.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: Tensor([1. 2. 3.], requires_grad=True)\n",
      "Matrix: Tensor([[1. 2.]\n",
      " [3. 4.]], requires_grad=True)\n",
      "Random tensor: Tensor([[ 0.1874374   2.30901503 -2.09992575]\n",
      " [ 0.18774414  0.10507803  1.04678915]\n",
      " [-0.17617486  1.20022882  0.42736107]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Create tensors from various data types\n",
    "a = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = Tensor([[1, 2], [3, 4]], requires_grad=True)\n",
    "c = Tensor(np.random.randn(3, 3), requires_grad=True)\n",
    "\n",
    "print(f\"Vector: {a}\")\n",
    "print(f\"Matrix: {b}\")\n",
    "print(f\"Random tensor: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y = [5. 7. 9.]\n",
      "x * y = [ 4. 10. 18.]\n",
      "Matrix multiplication result:\n",
      "[[10.  7.]\n",
      " [22. 15.]]\n"
     ]
    }
   ],
   "source": [
    "# Basic arithmetic\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = Tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "\n",
    "# Addition and multiplication\n",
    "z = x + y\n",
    "w = x * y\n",
    "\n",
    "print(f\"x + y = {z.data}\")\n",
    "print(f\"x * y = {w.data}\")\n",
    "\n",
    "# Matrix operations\n",
    "A = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "B = Tensor([[2.0, 1.0], [4.0, 3.0]], requires_grad=True)\n",
    "C = A @ B  # Matrix multiplication\n",
    "\n",
    "print(f\"Matrix multiplication result:\\n{C.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations & Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic DIfferentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0) = 9.0\n",
      "f'(2.0) = 6.0\n",
      "Expected derivative: 6.0\n"
     ]
    }
   ],
   "source": [
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Function: f(x) = xÂ² + 2x + 1\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x * x + 2 * x + 1\n",
    "print(f\"f({x.data[0]}) = {y.data[0]}\")\n",
    "\n",
    "# Backward pass - compute gradient\n",
    "y.backward()\n",
    "print(f\"f'({x.data[0]}) = {x.grad[0]}\")\n",
    "print(f\"Expected derivative: {2 * x.data[0] + 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a = [3. 4.]\n",
      "Gradient of b = [1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Function: f(a, b) = sum(a * b)\n",
    "a = Tensor([1.0, 2.0], requires_grad=True)\n",
    "b = Tensor([3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "c = a * b\n",
    "loss = c.sum()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Gradient of a = {a.grad}\")\n",
    "print(f\"Gradient of b = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain rule gradient: 20.0\n"
     ]
    }
   ],
   "source": [
    "# Complex function composition\n",
    "x = Tensor([1.0], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y + 3\n",
    "w = z * z\n",
    "\n",
    "w.backward()\n",
    "print(f\"Chain rule gradient: {x.grad[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 3)\n",
      "Output shape: (1, 2)\n",
      "Layer weights shape: (2, 3)\n",
      "Layer bias shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.linear import Linear\n",
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Create a linear layer\n",
    "layer = Linear(3, 2)  # 3 inputs -> 2 outputs\n",
    "\n",
    "# Forward pass\n",
    "x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "output = layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.data.shape}\")\n",
    "print(f\"Output shape: {output.data.shape}\")\n",
    "print(f\"Layer weights shape: {layer.weight.data.shape}\")\n",
    "print(f\"Layer bias shape: {layer.bias.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[-2. -1.  0.  1.  2.]]\n",
      "ReLU: [[-0. -0.  0.  1.  2.]]\n",
      "Softmax: [[0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]]\n",
      "GELU: [[-0.04540231 -0.15880801  0.          0.84119199  1.95459769]]\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.activation import ReLU, Softmax, GELU\n",
    "\n",
    "# Test different activations\n",
    "x = Tensor([[-2.0, -1.0, 0.0, 1.0, 2.0]], requires_grad=True)\n",
    "\n",
    "relu = ReLU()\n",
    "softmax = Softmax()\n",
    "gelu = GELU()\n",
    "\n",
    "print(f\"Input: {x.data}\")\n",
    "print(f\"ReLU: {relu(x).data}\")\n",
    "print(f\"Softmax: {softmax(x).data}\")\n",
    "print(f\"GELU: {gelu(x).data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [[0.01665713 0.93909529 0.04424758]]\n",
      "Output sum (should be ~1.0): 1.0\n",
      "Number of parameters: 6\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.container import Sequential\n",
    "from fit.nn.modules.linear import Linear\n",
    "from fit.nn.modules.activation import ReLU, Softmax\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential(\n",
    "    Linear(4, 8),    # Input layer\n",
    "    ReLU(),          # Activation\n",
    "    Linear(8, 6),    # Hidden layer\n",
    "    ReLU(),          # Activation\n",
    "    Linear(6, 3),    # Output layer\n",
    "    Softmax()        # Final activation\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "x = Tensor([[1.0, 2.0, 3.0, 4.0]], requires_grad=True)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Model output: {output.data}\")\n",
    "print(f\"Output sum (should be ~1.0): {output.data.sum()}\")\n",
    "print(f\"Number of parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [[1. 2. 3. 4.]\n",
      " [2. 3. 4. 5.]]\n",
      "Batch normalized: [[-0.99998 -0.99998 -0.99998 -0.99998]\n",
      " [ 0.99998  0.99998  0.99998  0.99998]]\n",
      "Layer normalized: [[-1.34163542 -0.44721181  0.44721181  1.34163542]\n",
      " [-1.34163542 -0.44721181  0.44721181  1.34163542]]\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.normalization import BatchNorm, LayerNorm\n",
    "\n",
    "# Batch normalization\n",
    "batch_norm = BatchNorm(4)\n",
    "x = Tensor([[1.0, 2.0, 3.0, 4.0], [2.0, 3.0, 4.0, 5.0]], requires_grad=True)\n",
    "normalized = batch_norm(x)\n",
    "\n",
    "print(f\"Original: {x.data}\")\n",
    "print(f\"Batch normalized: {normalized.data}\")\n",
    "\n",
    "# Layer normalization  \n",
    "layer_norm = LayerNorm(4)\n",
    "layer_normalized = layer_norm(x)\n",
    "print(f\"Layer normalized: {layer_normalized.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SGD.__init__() got an unexpected keyword argument 'momentum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create model and optimizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Linear(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Simulate training step\u001b[39;00m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m Tensor([[\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m2.0\u001b[39m]], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: SGD.__init__() got an unexpected keyword argument 'momentum'"
     ]
    }
   ],
   "source": [
    "from fit.optim.sgd import SGD\n",
    "from fit.nn.modules.linear import Linear\n",
    "\n",
    "# Create model and optimizer\n",
    "model = Linear(2, 1)\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Simulate training step\n",
    "x = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "target = Tensor([[3.0]])\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "loss = ((output - target) ** 2).mean()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Optimizer step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(f\"Loss: {loss.data}\")\n",
    "print(\"Training step completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.optim.adam import Adam\n",
    "\n",
    "# Create model and Adam optimizer\n",
    "model = Sequential(\n",
    "    Linear(2, 4),\n",
    "    ReLU(),\n",
    "    Linear(4, 1)\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, beta1=0.9, beta2=0.999)\n",
    "\n",
    "# Training loop simulation\n",
    "for step in range(5):\n",
    "    # Sample data\n",
    "    x = Tensor([[np.random.randn(), np.random.randn()]], requires_grad=True)\n",
    "    target = Tensor([[np.random.randn()]])\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    loss = ((output - target) ** 2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"Step {step+1}, Loss: {loss.data[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.data.dataset import Dataset\n",
    "from fit.data.dataloader import DataLoader\n",
    "\n",
    "# Create custom dataset\n",
    "X = np.random.randn(100, 4)\n",
    "y = np.random.randint(0, 3, 100)\n",
    "\n",
    "dataset = Dataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Iterate through batches\n",
    "for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "    print(f\"Batch {i+1}: X shape {batch_x.data.shape}, y shape {batch_y.data.shape}\")\n",
    "    if i >= 2:  # Show first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.simple.data import load_dataset\n",
    "\n",
    "# Load XOR dataset\n",
    "xor_data = load_dataset('xor', batch_size=4)\n",
    "print(\"XOR Dataset loaded:\")\n",
    "print(f\"Train loader batches: {len(xor_data['train'])}\")\n",
    "\n",
    "# Load Iris dataset\n",
    "iris_data = load_dataset('iris', batch_size=32, validation_split=0.2)\n",
    "print(\"Iris Dataset loaded:\")\n",
    "print(f\"Train batches: {len(iris_data['train'])}\")\n",
    "print(f\"Validation batches: {len(iris_data['val'])}\")\n",
    "\n",
    "# Sample from XOR dataset\n",
    "for x, y in xor_data['train']:\n",
    "    print(f\"XOR - X: {x.data}, y: {y.data}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.data.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.randn(100, 10)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "print(f\"Selected feature indices: {selector.get_support()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.loss.regression import MSELoss\n",
    "\n",
    "mse = MSELoss()\n",
    "\n",
    "# Sample predictions and targets\n",
    "predictions = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "targets = Tensor([[1.2, 1.8], [2.9, 4.1]])\n",
    "\n",
    "loss = mse(predictions, targets)\n",
    "print(f\"MSE Loss: {loss.data}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(f\"Gradients: {predictions.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.loss.classification import CrossEntropyLoss\n",
    "\n",
    "ce_loss = CrossEntropyLoss()\n",
    "\n",
    "# Logits and class labels\n",
    "logits = Tensor([[2.0, 1.0, 0.1], [0.5, 2.0, 0.8]], requires_grad=True)\n",
    "targets = Tensor([0, 1])  # Class indices\n",
    "\n",
    "loss = ce_loss(logits, targets)\n",
    "print(f\"CrossEntropy Loss: {loss.data}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(f\"Logits gradients: {logits.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention & Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.nn.modules.attention import MultiHeadAttention\n",
    "\n",
    "# Create multi-head attention\n",
    "attention = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "\n",
    "# Sample input (batch_size=2, seq_len=10, d_model=64)\n",
    "x = Tensor(np.random.randn(2, 10, 64), requires_grad=True)\n",
    "\n",
    "# Self-attention\n",
    "output = attention(x, x, x)\n",
    "print(f\"Attention input shape: {x.data.shape}\")\n",
    "print(f\"Attention output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.nn.modules.transformer import TransformerEncoderBlock\n",
    "\n",
    "# Create transformer encoder block\n",
    "transformer_block = TransformerEncoderBlock(\n",
    "    d_model=64, \n",
    "    num_heads=8, \n",
    "    d_ff=256, \n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "x = Tensor(np.random.randn(2, 10, 64), requires_grad=True)\n",
    "output = transformer_block(x)\n",
    "\n",
    "print(f\"Transformer input shape: {x.data.shape}\")\n",
    "print(f\"Transformer output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.nn.utils.spectral_norm import SpectralNormLinear\n",
    "\n",
    "# Linear layer with spectral normalization\n",
    "spec_linear = SpectralNormLinear(10, 5, n_power_iterations=1)\n",
    "\n",
    "x = Tensor(np.random.randn(3, 10), requires_grad=True)\n",
    "output = spec_linear(x)\n",
    "\n",
    "print(f\"Output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.monitor.tracker import TrainingTracker\n",
    "\n",
    "# Create tracker\n",
    "tracker = TrainingTracker(experiment_name=\"demo_experiment\")\n",
    "\n",
    "# Simulate training with metrics\n",
    "for epoch in range(10):\n",
    "    # Simulate epoch metrics\n",
    "    train_loss = 1.0 - epoch * 0.1 + np.random.normal(0, 0.05)\n",
    "    val_loss = 1.2 - epoch * 0.08 + np.random.normal(0, 0.08)\n",
    "    accuracy = epoch * 0.08 + np.random.normal(0, 0.02)\n",
    "    \n",
    "    # Log metrics\n",
    "    tracker.log_epoch(epoch, {\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    tracker.log_learning_rate(0.001 * (0.9 ** epoch))\n",
    "\n",
    "print(\"Metrics logged!\")\n",
    "print(f\"Best validation loss: {tracker.best_values.get('val_loss', 'N/A')}\")\n",
    "\n",
    "# Export metrics\n",
    "tracker.export(\"training_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.utils.engine import evaluate\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "model = Sequential(\n",
    "    Linear(4, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 3),\n",
    "    Softmax()\n",
    ")\n",
    "\n",
    "# Create sample data\n",
    "X = Tensor(np.random.randn(20, 4))\n",
    "y = Tensor(np.random.randint(0, 3, 20))\n",
    "dataset = Dataset(X.data, y.data)\n",
    "dataloader = DataLoader(dataset, batch_size=5)\n",
    "\n",
    "# Evaluate model\n",
    "from fit.loss.classification import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "metrics = evaluate(model, dataloader, loss_fn)\n",
    "print(f\"Evaluation metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model save/load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.nn.utils.model_io import save_model, load_model\n",
    "\n",
    "# Create and train a simple model\n",
    "model = Sequential(\n",
    "    Linear(2, 4),\n",
    "    ReLU(),\n",
    "    Linear(4, 1)\n",
    ")\n",
    "\n",
    "# Save model\n",
    "save_model(model, \"demo_model.pkl\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = load_model(\"demo_model.pkl\")\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Test that loaded model works\n",
    "test_input = Tensor([[1.0, 2.0]])\n",
    "original_output = model(test_input)\n",
    "loaded_output = loaded_model(test_input)\n",
    "\n",
    "print(f\"Original output: {original_output.data}\")\n",
    "print(f\"Loaded output: {loaded_output.data}\")\n",
    "print(f\"Outputs match: {np.allclose(original_output.data, loaded_output.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.simple.trainer import Trainer\n",
    "\n",
    "# Create XOR model\n",
    "xor_model = Sequential(\n",
    "    Linear(2, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 4),\n",
    "    ReLU(),\n",
    "    Linear(4, 1)\n",
    ")\n",
    "\n",
    "# XOR data\n",
    "X = Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = Tensor([[0], [1], [1], [0]])\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=xor_model,\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training XOR model...\")\n",
    "history = trainer.fit(\n",
    "    data=(X, y),\n",
    "    epochs=100,\n",
    "    batch_size=4,\n",
    "    validation_split=0.0,  # Use all data for training\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the trained model\n",
    "print(\"\\nXOR Results:\")\n",
    "for i, (input_val, expected) in enumerate(zip(X.data, y.data)):\n",
    "    prediction = xor_model(Tensor([input_val]))\n",
    "    print(f\"Input: {input_val}, Expected: {expected[0]:.0f}, Predicted: {prediction.data[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris_data = load_dataset('iris', batch_size=16, validation_split=0.3)\n",
    "\n",
    "# Create classification model\n",
    "classifier = Sequential(\n",
    "    Linear(4, 16),\n",
    "    ReLU(),\n",
    "    Linear(16, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 3),\n",
    "    Softmax()\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=classifier,\n",
    "    loss='crossentropy',\n",
    "    optimizer='adam',\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Training Iris classifier...\")\n",
    "history = trainer.fit(\n",
    "    data=iris_data['train'],\n",
    "    validation_data=iris_data['val'],\n",
    "    epochs=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "final_metrics = evaluate(classifier, iris_data['val'], CrossEntropyLoss())\n",
    "print(f\"Final validation metrics: {final_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.optim.adam import Adam\n",
    "from fit.loss.regression import MSELoss\n",
    "\n",
    "# Generate synthetic regression data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(100, 3)\n",
    "true_weights = np.array([1.5, -2.0, 0.5])\n",
    "y_train = X_train @ true_weights + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Create model\n",
    "regression_model = Sequential(\n",
    "    Linear(3, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 1)\n",
    ")\n",
    "\n",
    "# Setup training\n",
    "optimizer = Adam(regression_model.parameters(), lr=0.01)\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "# Custom training loop\n",
    "print(\"Training loop for regression...\")\n",
    "for epoch in range(100):\n",
    "    # Convert to tensors\n",
    "    X_tensor = Tensor(X_train, requires_grad=True)\n",
    "    y_tensor = Tensor(y_train.reshape(-1, 1))\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = regression_model(X_tensor)\n",
    "    loss = loss_fn(predictions, y_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.data[0]:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Test prediction\n",
    "test_X = Tensor([[1.0, -1.0, 0.5]])\n",
    "prediction = regression_model(test_X)\n",
    "expected = 1.0 * 1.5 + (-1.0) * (-2.0) + 0.5 * 0.5  # Using true weights\n",
    "print(f\"Test prediction: {prediction.data[0]:.3f}, Expected: {expected:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
