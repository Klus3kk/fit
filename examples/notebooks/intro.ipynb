{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT Library Showcase\n",
    "\n",
    "This notebook demonstrates the features of the FIT library - a machine learning framework built with the help of NumPy.\n",
    "\n",
    "The FIT library that I've created provides tensor operations with automatic differentiation, neural network components with inclusion of linear layers, activations, and normalization, attention mechanisms and transformers, multiple optimizers such as SGD, Adam, SAM, and Lion, loss functions for regression and classification, data pipeline utilities with built-in datasets, training monitoring and visualization, model persistence for saving and loading, and APIs for quick experimentation.\n",
    "\n",
    "I tried to make this library lightweight and it only requires NumPy. It offers a PyTorch-like familiar API, and has an extensible clean architecture for adding components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/Klus3kk/fit.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: Tensor([1. 2. 3.], requires_grad=True)\n",
      "Matrix: Tensor([[1. 2.]\n",
      " [3. 4.]], requires_grad=True)\n",
      "Random tensor: Tensor([[-1.08957361 -1.14300669 -1.06784566]\n",
      " [ 0.33019738 -0.22200625 -1.82231365]\n",
      " [ 0.33581401  1.90352181 -0.31104841]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Create tensors from various data types\n",
    "a = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = Tensor([[1, 2], [3, 4]], requires_grad=True)\n",
    "c = Tensor(np.random.randn(3, 3), requires_grad=True)\n",
    "\n",
    "print(f\"Vector: {a}\")\n",
    "print(f\"Matrix: {b}\")\n",
    "print(f\"Random tensor: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y = [5. 7. 9.]\n",
      "x * y = [ 4. 10. 18.]\n",
      "Matrix multiplication result:\n",
      "[[10.  7.]\n",
      " [22. 15.]]\n"
     ]
    }
   ],
   "source": [
    "# Basic arithmetic\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = Tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "\n",
    "# Addition and multiplication\n",
    "z = x + y\n",
    "w = x * y\n",
    "\n",
    "print(f\"x + y = {z.data}\")\n",
    "print(f\"x * y = {w.data}\")\n",
    "\n",
    "# Matrix operations\n",
    "A = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "B = Tensor([[2.0, 1.0], [4.0, 3.0]], requires_grad=True)\n",
    "C = A @ B  # Matrix multiplication\n",
    "\n",
    "print(f\"Matrix multiplication result:\\n{C.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations & Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic DIfferentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0) = 9.0\n",
      "f'(2.0) = 6.0\n",
      "Expected derivative: 6.0\n"
     ]
    }
   ],
   "source": [
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Function: f(x) = xÂ² + 2x + 1\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x * x + 2 * x + 1\n",
    "print(f\"f({x.data[0]}) = {y.data[0]}\")\n",
    "\n",
    "# Backward pass - compute gradient\n",
    "y.backward()\n",
    "print(f\"f'({x.data[0]}) = {x.grad[0]}\")\n",
    "print(f\"Expected derivative: {2 * x.data[0] + 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a = [3. 4.]\n",
      "Gradient of b = [1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Function: f(a, b) = sum(a * b)\n",
    "a = Tensor([1.0, 2.0], requires_grad=True)\n",
    "b = Tensor([3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "c = a * b\n",
    "loss = c.sum()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Gradient of a = {a.grad}\")\n",
    "print(f\"Gradient of b = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain rule gradient: 20.0\n"
     ]
    }
   ],
   "source": [
    "# Complex function composition\n",
    "x = Tensor([1.0], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y + 3\n",
    "w = z * z\n",
    "\n",
    "w.backward()\n",
    "print(f\"Chain rule gradient: {x.grad[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 3)\n",
      "Output shape: (1, 2)\n",
      "Layer weights shape: (2, 3)\n",
      "Layer bias shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.linear import Linear\n",
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Create a linear layer\n",
    "layer = Linear(3, 2)  # 3 inputs -> 2 outputs\n",
    "\n",
    "# Forward pass\n",
    "x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "output = layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.data.shape}\")\n",
    "print(f\"Output shape: {output.data.shape}\")\n",
    "print(f\"Layer weights shape: {layer.weight.data.shape}\")\n",
    "print(f\"Layer bias shape: {layer.bias.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[-2. -1.  0.  1.  2.]]\n",
      "ReLU: [[-0. -0.  0.  1.  2.]]\n",
      "Softmax: [[0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]]\n",
      "GELU: [[-0.04540231 -0.15880801  0.          0.84119199  1.95459769]]\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.activation import ReLU, Softmax, GELU\n",
    "\n",
    "# Test different activations\n",
    "x = Tensor([[-2.0, -1.0, 0.0, 1.0, 2.0]], requires_grad=True)\n",
    "\n",
    "relu = ReLU()\n",
    "softmax = Softmax()\n",
    "gelu = GELU()\n",
    "\n",
    "print(f\"Input: {x.data}\")\n",
    "print(f\"ReLU: {relu(x).data}\")\n",
    "print(f\"Softmax: {softmax(x).data}\")\n",
    "print(f\"GELU: {gelu(x).data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [[0.70130264 0.12593192 0.17276544]]\n",
      "Output sum (should be ~1.0): 1.0\n",
      "Number of parameters: 6\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.container import Sequential\n",
    "from fit.nn.modules.linear import Linear\n",
    "from fit.nn.modules.activation import ReLU, Softmax\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential(\n",
    "    Linear(4, 8),  # Input layer\n",
    "    ReLU(),  # Activation\n",
    "    Linear(8, 6),  # Hidden layer\n",
    "    ReLU(),  # Activation\n",
    "    Linear(6, 3),  # Output layer\n",
    "    Softmax(),  # Final activation\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "x = Tensor([[1.0, 2.0, 3.0, 4.0]], requires_grad=True)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Model output: {output.data}\")\n",
    "print(f\"Output sum (should be ~1.0): {output.data.sum()}\")\n",
    "print(f\"Number of parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [[1. 2. 3. 4.]\n",
      " [2. 3. 4. 5.]]\n",
      "Batch normalized: [[-0.99998 -0.99998 -0.99998 -0.99998]\n",
      " [ 0.99998  0.99998  0.99998  0.99998]]\n",
      "Layer normalized: [[-1.34163542 -0.44721181  0.44721181  1.34163542]\n",
      " [-1.34163542 -0.44721181  0.44721181  1.34163542]]\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.normalization import BatchNorm, LayerNorm\n",
    "\n",
    "# Batch normalization\n",
    "batch_norm = BatchNorm(4)\n",
    "x = Tensor([[1.0, 2.0, 3.0, 4.0], [2.0, 3.0, 4.0, 5.0]], requires_grad=True)\n",
    "normalized = batch_norm(x)\n",
    "\n",
    "print(f\"Original: {x.data}\")\n",
    "print(f\"Batch normalized: {normalized.data}\")\n",
    "\n",
    "# Layer normalization\n",
    "layer_norm = LayerNorm(4)\n",
    "layer_normalized = layer_norm(x)\n",
    "print(f\"Layer normalized: {layer_normalized.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.835619504464079\n",
      "Training step completed!\n"
     ]
    }
   ],
   "source": [
    "from fit.optim.sgd import SGD\n",
    "from fit.nn.modules.linear import Linear\n",
    "\n",
    "# Create model and optimizer\n",
    "model = Linear(2, 1)\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Simulate training step\n",
    "x = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "target = Tensor([[3.0]])\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "loss = ((output - target) ** 2).mean()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Optimizer step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(f\"Loss: {loss.data}\")\n",
    "print(\"Training step completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 0.8734\n",
      "Step 2, Loss: 0.4638\n",
      "Step 3, Loss: 0.8612\n",
      "Step 4, Loss: 1.1857\n",
      "Step 5, Loss: 0.6425\n"
     ]
    }
   ],
   "source": [
    "from fit.optim.adam import Adam\n",
    "\n",
    "# Create model and Adam optimizer\n",
    "model = Sequential(Linear(2, 4), ReLU(), Linear(4, 1))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# Training loop simulation\n",
    "for step in range(5):\n",
    "    # Sample data\n",
    "    x = Tensor([[np.random.randn(), np.random.randn()]], requires_grad=True)\n",
    "    target = Tensor([[np.random.randn()]])\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    loss = ((output - target) ** 2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Fix: Handle both scalar and array loss values\n",
    "    if loss.data.ndim == 0:\n",
    "        # Scalar value\n",
    "        loss_value = float(loss.data)\n",
    "    else:\n",
    "        # Array value\n",
    "        loss_value = float(loss.data[0])\n",
    "\n",
    "    print(f\"Step {step+1}, Loss: {loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100\n",
      "Number of batches: 7\n",
      "Batch 1: X shape (16, 4), y shape (16,)\n",
      "Batch 2: X shape (16, 4), y shape (16,)\n",
      "Batch 3: X shape (16, 4), y shape (16,)\n"
     ]
    }
   ],
   "source": [
    "from fit.data.dataset import Dataset\n",
    "from fit.data.dataloader import DataLoader\n",
    "\n",
    "# Create custom dataset\n",
    "X = np.random.randn(100, 4)\n",
    "y = np.random.randint(0, 3, 100)\n",
    "\n",
    "dataset = Dataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Iterate through batches\n",
    "for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "    print(f\"Batch {i+1}: X shape {batch_x.data.shape}, y shape {batch_y.data.shape}\")\n",
    "    if i >= 2:  # Show first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating XOR dataset...\n",
      "XOR Dataset loaded:\n",
      "Train loader batches: 250\n",
      "Loading Iris dataset...\n",
      "Iris Dataset loaded:\n",
      "Train batches: 4\n",
      "Validation batches: 1\n",
      "XOR - X: [[0. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]], y: [0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from fit.simple.data import load_dataset\n",
    "\n",
    "# Load XOR dataset\n",
    "xor_data = load_dataset(\"xor\", batch_size=4)\n",
    "print(\"XOR Dataset loaded:\")\n",
    "print(f\"Train loader batches: {len(xor_data['train'])}\")\n",
    "\n",
    "# Load Iris dataset\n",
    "iris_data = load_dataset(\"iris\", batch_size=32, validation_split=0.2)\n",
    "print(\"Iris Dataset loaded:\")\n",
    "print(f\"Train batches: {len(iris_data['train'])}\")\n",
    "print(f\"Validation batches: {len(iris_data['val'])}\")\n",
    "\n",
    "# Sample from XOR dataset\n",
    "for x, y in xor_data[\"train\"]:\n",
    "    print(f\"XOR - X: {x.data}, y: {y.data}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 10\n",
      "Selected features: 5\n",
      "Selected feature indices: [ True False  True False False  True  True False False  True]\n"
     ]
    }
   ],
   "source": [
    "from fit.data.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.randn(100, 10)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "print(f\"Selected feature indices: {selector.get_support()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.024999999999999977\n",
      "Gradients: [[-0.1   0.1 ]\n",
      " [ 0.05 -0.05]]\n"
     ]
    }
   ],
   "source": [
    "from fit.loss.regression import MSELoss\n",
    "\n",
    "mse = MSELoss()\n",
    "\n",
    "# Sample predictions and targets\n",
    "predictions = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "targets = Tensor([[1.2, 1.8], [2.9, 4.1]])\n",
    "\n",
    "loss = mse(predictions, targets)\n",
    "print(f\"MSE Loss: {loss.data}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(f\"Gradients: {predictions.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy Loss: 0.419290646726997\n",
      "Logits gradients: [[-0.17049943  0.12121649  0.04928295]\n",
      " [ 0.07318986 -0.17198583  0.09879597]]\n"
     ]
    }
   ],
   "source": [
    "from fit.loss.classification import CrossEntropyLoss\n",
    "\n",
    "ce_loss = CrossEntropyLoss()\n",
    "\n",
    "# Logits and class labels\n",
    "logits = Tensor([[2.0, 1.0, 0.1], [0.5, 2.0, 0.8]], requires_grad=True)\n",
    "targets = Tensor([0, 1])  # Class indices\n",
    "\n",
    "loss = ce_loss(logits, targets)\n",
    "print(f\"CrossEntropy Loss: {loss.data}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(f\"Logits gradients: {logits.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention & Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention input shape: (2, 10, 64)\n",
      "Attention output shape: (2, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.attention import MultiHeadAttention\n",
    "\n",
    "# Create multi-head attention\n",
    "attention = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "\n",
    "# Sample input (batch_size=2, seq_len=10, d_model=64)\n",
    "x = Tensor(np.random.randn(2, 10, 64), requires_grad=True)\n",
    "\n",
    "# Self-attention\n",
    "output = attention(x, x, x)\n",
    "print(f\"Attention input shape: {x.data.shape}\")\n",
    "print(f\"Attention output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer input shape: (2, 10, 64)\n",
      "Transformer output shape: (2, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.transformer import TransformerEncoderBlock\n",
    "\n",
    "# Create transformer encoder block\n",
    "transformer_block = TransformerEncoderBlock(\n",
    "    d_model=64, num_heads=8, d_ff=256, dropout=0.1\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "x = Tensor(np.random.randn(2, 10, 64), requires_grad=True)\n",
    "output = transformer_block(x)\n",
    "\n",
    "print(f\"Transformer input shape: {x.data.shape}\")\n",
    "print(f\"Transformer output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.utils.spectral_norm import SpectralNormLinear\n",
    "\n",
    "# Linear layer with spectral normalization\n",
    "spec_linear = SpectralNormLinear(10, 5, n_power_iterations=1)\n",
    "\n",
    "x = Tensor(np.random.randn(3, 10), requires_grad=True)\n",
    "output = spec_linear(x)\n",
    "\n",
    "print(f\"Output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train_loss  val_loss    accuracy    Time    \n",
      "--------------------------------------------------\n",
      "1     0.9389      0.9989      0.0152      \n",
      "2     0.8907      1.1735      0.0667      \n",
      "3     0.8214      1.1364      0.1414      \n",
      "4     0.7042      0.9471      0.2042      \n",
      "5     0.5579      0.8620      0.3140      \n",
      "6     0.5659      0.7661      0.3624      \n",
      "7     0.3904      0.8223      0.4745      \n",
      "8     0.3932      0.5757      0.5251      \n",
      "9     0.3083      0.4118      0.6616      \n",
      "10    0.1303      0.5031      0.7300      \n",
      "Metrics logged!\n",
      "Best validation loss: 0.41183470041323994\n",
      "Logs exported to training_log.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'training_log.json'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fit.monitor.tracker import TrainingTracker\n",
    "\n",
    "# Create tracker\n",
    "tracker = TrainingTracker(experiment_name=\"demo_experiment\")\n",
    "\n",
    "# Simulate training with metrics\n",
    "for epoch in range(10):\n",
    "    # Simulate epoch metrics\n",
    "    train_loss = 1.0 - epoch * 0.1 + np.random.normal(0, 0.05)\n",
    "    val_loss = 1.2 - epoch * 0.08 + np.random.normal(0, 0.08)\n",
    "    accuracy = epoch * 0.08 + np.random.normal(0, 0.02)\n",
    "\n",
    "    # Log metrics\n",
    "    tracker.update(\n",
    "        epoch, {\"train_loss\": train_loss, \"val_loss\": val_loss, \"accuracy\": accuracy}\n",
    "    )\n",
    "\n",
    "    tracker.log_learning_rate(0.001 * (0.9**epoch))\n",
    "\n",
    "print(\"Metrics logged!\")\n",
    "print(f\"Best validation loss: {tracker.best_values.get('val_loss', 'N/A')}\")\n",
    "\n",
    "# Export metrics\n",
    "tracker.export(\"training_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'loss': np.float64(1.0865359454049197), 'accuracy': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "from fit.utils.engine import evaluate\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "model = Sequential(Linear(4, 8), ReLU(), Linear(8, 3), Softmax())\n",
    "\n",
    "# Create sample data\n",
    "X = Tensor(np.random.randn(20, 4))\n",
    "y = Tensor(np.random.randint(0, 3, 20))\n",
    "dataset = Dataset(X.data, y.data)\n",
    "dataloader = DataLoader(dataset, batch_size=5)\n",
    "\n",
    "# Evaluate model\n",
    "from fit.loss.classification import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "metrics = evaluate(model, dataloader, loss_fn)\n",
    "print(f\"Evaluation metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model save/load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to demo_model.pkl\n",
      "Model saved!\n",
      "Model loaded from demo_model.pkl\n",
      "Model loaded!\n",
      "Original output: [[-0.21497577]]\n",
      "Loaded output: [[-0.21497577]]\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.utils.model_io import save_model, load_model\n",
    "\n",
    "# Create and train a simple model\n",
    "model = Sequential(Linear(2, 4), ReLU(), Linear(4, 1))\n",
    "\n",
    "# Save model\n",
    "save_model(model, \"demo_model.pkl\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = load_model(\"demo_model.pkl\")\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Test that loaded model works\n",
    "test_input = Tensor([[1.0, 2.0]])\n",
    "original_output = model(test_input)\n",
    "loaded_output = loaded_model(test_input)\n",
    "\n",
    "print(f\"Original output: {original_output.data}\")\n",
    "print(f\"Loaded output: {loaded_output.data}\")\n",
    "print(f\"Outputs match: {np.allclose(original_output.data, loaded_output.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XOR model...\n",
      "Starting training for 100 epochs...\n",
      "Model: Sequential\n",
      "Optimizer: Adam\n",
      "Loss: MSELoss\n",
      "Batch size: 32\n",
      "--------------------------------------------------\n",
      "Epoch train_loss  Time    \n",
      "--------------------------\n",
      "1     0.5087      \n",
      "Epoch 1: train_loss=0.5087\n",
      "2     0.4190      \n",
      "Epoch 2: train_loss=0.4190\n",
      "3     0.3445      \n",
      "Epoch 3: train_loss=0.3445\n",
      "4     0.2844      \n",
      "Epoch 4: train_loss=0.2844\n",
      "5     0.2368      \n",
      "Epoch 5: train_loss=0.2368\n",
      "6     0.1989      \n",
      "Epoch 6: train_loss=0.1989\n",
      "7     0.1719      \n",
      "Epoch 7: train_loss=0.1719\n",
      "8     0.1514      \n",
      "Epoch 8: train_loss=0.1514\n",
      "9     0.1336      \n",
      "Epoch 9: train_loss=0.1336\n",
      "10    0.1180      \n",
      "Epoch 10: train_loss=0.1180\n",
      "11    0.1040      \n",
      "Epoch 11: train_loss=0.1040\n",
      "12    0.0917      \n",
      "Epoch 12: train_loss=0.0917\n",
      "13    0.0811      \n",
      "Epoch 13: train_loss=0.0811\n",
      "14    0.0721      \n",
      "Epoch 14: train_loss=0.0721\n",
      "15    0.0646      \n",
      "Epoch 15: train_loss=0.0646\n",
      "16    0.0587      \n",
      "Epoch 16: train_loss=0.0587\n",
      "17    0.0538      \n",
      "Epoch 17: train_loss=0.0538\n",
      "18    0.0497      \n",
      "Epoch 18: train_loss=0.0497\n",
      "19    0.0459      \n",
      "Epoch 19: train_loss=0.0459\n",
      "20    0.0420      \n",
      "Epoch 20: train_loss=0.0420\n",
      "21    0.0380      \n",
      "Epoch 21: train_loss=0.0380\n",
      "22    0.0337      \n",
      "Epoch 22: train_loss=0.0337\n",
      "23    0.0295      \n",
      "Epoch 23: train_loss=0.0295\n",
      "24    0.0254      \n",
      "Epoch 24: train_loss=0.0254\n",
      "25    0.0224      \n",
      "Epoch 25: train_loss=0.0224\n",
      "26    0.0195      \n",
      "Epoch 26: train_loss=0.0195\n",
      "27    0.0167      \n",
      "Epoch 27: train_loss=0.0167\n",
      "28    0.0142      \n",
      "Epoch 28: train_loss=0.0142\n",
      "29    0.0124      \n",
      "Epoch 29: train_loss=0.0124\n",
      "30    0.0108      \n",
      "Epoch 30: train_loss=0.0108\n",
      "31    0.0091      \n",
      "Epoch 31: train_loss=0.0091\n",
      "32    0.0075      \n",
      "Epoch 32: train_loss=0.0075\n",
      "33    0.0059      \n",
      "Epoch 33: train_loss=0.0059\n",
      "34    0.0046      \n",
      "Epoch 34: train_loss=0.0046\n",
      "35    0.0035      \n",
      "Epoch 35: train_loss=0.0035\n",
      "36    0.0026      \n",
      "Epoch 36: train_loss=0.0026\n",
      "37    0.0018      \n",
      "Epoch 37: train_loss=0.0018\n",
      "38    0.0013      \n",
      "Epoch 38: train_loss=0.0013\n",
      "39    0.0009      \n",
      "Epoch 39: train_loss=0.0009\n",
      "40    0.0006      \n",
      "Epoch 40: train_loss=0.0006\n",
      "41    0.0004      \n",
      "Epoch 41: train_loss=0.0004\n",
      "42    0.0003      \n",
      "Epoch 42: train_loss=0.0003\n",
      "43    0.0003      \n",
      "Epoch 43: train_loss=0.0003\n",
      "44    0.0003      \n",
      "Epoch 44: train_loss=0.0003\n",
      "45    0.0004      \n",
      "Epoch 45: train_loss=0.0004\n",
      "46    0.0005      \n",
      "Epoch 46: train_loss=0.0005\n",
      "47    0.0006      \n",
      "Epoch 47: train_loss=0.0006\n",
      "48    0.0007      \n",
      "Epoch 48: train_loss=0.0007\n",
      "49    0.0007      \n",
      "Epoch 49: train_loss=0.0007\n",
      "50    0.0007      \n",
      "Epoch 50: train_loss=0.0007\n",
      "51    0.0006      \n",
      "Epoch 51: train_loss=0.0006\n",
      "52    0.0006      \n",
      "Epoch 52: train_loss=0.0006\n",
      "53    0.0005      \n",
      "Epoch 53: train_loss=0.0005\n",
      "54    0.0004      \n",
      "Epoch 54: train_loss=0.0004\n",
      "55    0.0003      \n",
      "Epoch 55: train_loss=0.0003\n",
      "56    0.0003      \n",
      "Epoch 56: train_loss=0.0003\n",
      "57    0.0002      \n",
      "Epoch 57: train_loss=0.0002\n",
      "58    0.0002      \n",
      "Epoch 58: train_loss=0.0002\n",
      "59    0.0002      \n",
      "Epoch 59: train_loss=0.0002\n",
      "60    0.0001      \n",
      "Epoch 60: train_loss=0.0001\n",
      "61    0.0001      \n",
      "Epoch 61: train_loss=0.0001\n",
      "62    0.0001      \n",
      "Epoch 62: train_loss=0.0001\n",
      "63    0.0002      \n",
      "Epoch 63: train_loss=0.0002\n",
      "64    0.0002      \n",
      "Epoch 64: train_loss=0.0002\n",
      "65    0.0002      \n",
      "Epoch 65: train_loss=0.0002\n",
      "66    0.0002      \n",
      "Epoch 66: train_loss=0.0002\n",
      "67    0.0002      \n",
      "Epoch 67: train_loss=0.0002\n",
      "68    0.0002      \n",
      "Epoch 68: train_loss=0.0002\n",
      "69    0.0002      \n",
      "Epoch 69: train_loss=0.0002\n",
      "70    0.0001      \n",
      "Epoch 70: train_loss=0.0001\n",
      "71    0.0001      \n",
      "Epoch 71: train_loss=0.0001\n",
      "72    0.0001      \n",
      "Epoch 72: train_loss=0.0001\n",
      "73    0.0001      \n",
      "Epoch 73: train_loss=0.0001\n",
      "74    0.0001      \n",
      "Epoch 74: train_loss=0.0001\n",
      "75    0.0001      \n",
      "Epoch 75: train_loss=0.0001\n",
      "76    0.0001      \n",
      "Epoch 76: train_loss=0.0001\n",
      "77    0.0000      \n",
      "Epoch 77: train_loss=0.0000\n",
      "78    0.0000      \n",
      "Epoch 78: train_loss=0.0000\n",
      "79    0.0000      \n",
      "Epoch 79: train_loss=0.0000\n",
      "80    0.0000      \n",
      "Epoch 80: train_loss=0.0000\n",
      "81    0.0000      \n",
      "Epoch 81: train_loss=0.0000\n",
      "82    0.0000      \n",
      "Epoch 82: train_loss=0.0000\n",
      "83    0.0000      \n",
      "Epoch 83: train_loss=0.0000\n",
      "84    0.0000      \n",
      "Epoch 84: train_loss=0.0000\n",
      "85    0.0000      \n",
      "Epoch 85: train_loss=0.0000\n",
      "86    0.0000      \n",
      "Epoch 86: train_loss=0.0000\n",
      "87    0.0000      \n",
      "Epoch 87: train_loss=0.0000\n",
      "88    0.0000      \n",
      "Epoch 88: train_loss=0.0000\n",
      "89    0.0000      \n",
      "Epoch 89: train_loss=0.0000\n",
      "90    0.0000      \n",
      "Epoch 90: train_loss=0.0000\n",
      "91    0.0000      \n",
      "Epoch 91: train_loss=0.0000\n",
      "92    0.0000      \n",
      "Epoch 92: train_loss=0.0000\n",
      "93    0.0000      \n",
      "Epoch 93: train_loss=0.0000\n",
      "94    0.0000      \n",
      "Epoch 94: train_loss=0.0000\n",
      "95    0.0000      \n",
      "Epoch 95: train_loss=0.0000\n",
      "96    0.0000      \n",
      "Epoch 96: train_loss=0.0000\n",
      "97    0.0000      \n",
      "Epoch 97: train_loss=0.0000\n",
      "98    0.0000      \n",
      "Epoch 98: train_loss=0.0000\n",
      "99    0.0000      \n",
      "Epoch 99: train_loss=0.0000\n",
      "100   0.0000      \n",
      "Epoch 100: train_loss=0.0000\n",
      "Training completed!\n",
      "\n",
      "XOR Results:\n",
      "Input: [0. 0.], Expected: 0, Predicted: 0.003\n",
      "Input: [0. 1.], Expected: 1, Predicted: 0.997\n",
      "Input: [1. 0.], Expected: 1, Predicted: 1.001\n",
      "Input: [1. 1.], Expected: 0, Predicted: -0.004\n"
     ]
    }
   ],
   "source": [
    "from fit.simple.trainer import SimpleTrainer\n",
    "\n",
    "# Create XOR model\n",
    "xor_model = Sequential(Linear(2, 8), ReLU(), Linear(8, 4), ReLU(), Linear(4, 1))\n",
    "\n",
    "# XOR data\n",
    "X = Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = Tensor([[0], [1], [1], [0]])\n",
    "\n",
    "# Create trainer\n",
    "trainer = SimpleTrainer(\n",
    "    model=xor_model,\n",
    "    data=(X, y),\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training XOR model...\")\n",
    "history = trainer.fit(\n",
    "    epochs=100,          \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test the trained model\n",
    "print(\"\\nXOR Results:\")\n",
    "for i, (input_val, expected) in enumerate(zip(X.data, y.data)):\n",
    "    prediction = xor_model(Tensor([input_val]))\n",
    "    print(\n",
    "        f\"Input: {input_val}, Expected: {expected[0]:.0f}, Predicted: {float(prediction.data.flatten()[0]):.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Iris dataset...\n",
      "Training Iris classifier...\n",
      "Starting training for 50 epochs...\n",
      "Model: Sequential\n",
      "Optimizer: Adam\n",
      "Loss: CrossEntropyLoss\n",
      "Batch size: 16\n",
      "--------------------------------------------------\n",
      "Epoch train_loss  val_loss    val_accuracyTime    \n",
      "--------------------------------------------------\n",
      "1     0.9388      0.7904      0.7111      \n",
      "Epoch 1: train_loss=0.9388, val_loss=0.7904, val_acc=0.7111\n",
      "2     0.7772      0.7367      0.7556      \n",
      "Epoch 2: train_loss=0.7772, val_loss=0.7367, val_acc=0.7556\n",
      "3     0.7353      0.7237      0.8222      \n",
      "Epoch 3: train_loss=0.7353, val_loss=0.7237, val_acc=0.8222\n",
      "4     0.6990      0.7130      0.8222      \n",
      "Epoch 4: train_loss=0.6990, val_loss=0.7130, val_acc=0.8222\n",
      "5     0.6880      0.6977      0.8222      \n",
      "Epoch 5: train_loss=0.6880, val_loss=0.6977, val_acc=0.8222\n",
      "6     0.6643      0.6939      0.8444      \n",
      "Epoch 6: train_loss=0.6643, val_loss=0.6939, val_acc=0.8444\n",
      "7     0.6346      0.6692      0.8889      \n",
      "Epoch 7: train_loss=0.6346, val_loss=0.6692, val_acc=0.8889\n",
      "8     0.6180      0.6601      0.8889      \n",
      "Epoch 8: train_loss=0.6180, val_loss=0.6601, val_acc=0.8889\n",
      "9     0.6207      0.6591      0.9111      \n",
      "Epoch 9: train_loss=0.6207, val_loss=0.6591, val_acc=0.9111\n",
      "10    0.5999      0.6356      0.9111      \n",
      "Epoch 10: train_loss=0.5999, val_loss=0.6356, val_acc=0.9111\n",
      "11    0.5926      0.6311      0.9333      \n",
      "Epoch 11: train_loss=0.5926, val_loss=0.6311, val_acc=0.9333\n",
      "12    0.5865      0.6442      0.9333      \n",
      "Epoch 12: train_loss=0.5865, val_loss=0.6442, val_acc=0.9333\n",
      "13    0.5819      0.6296      0.9333      \n",
      "Epoch 13: train_loss=0.5819, val_loss=0.6296, val_acc=0.9333\n",
      "14    0.5740      0.6224      0.9333      \n",
      "Epoch 14: train_loss=0.5740, val_loss=0.6224, val_acc=0.9333\n",
      "15    0.5725      0.6266      0.9333      \n",
      "Epoch 15: train_loss=0.5725, val_loss=0.6266, val_acc=0.9333\n",
      "16    0.5698      0.6344      0.8889      \n",
      "Epoch 16: train_loss=0.5698, val_loss=0.6344, val_acc=0.8889\n",
      "17    0.5677      0.6385      0.9111      \n",
      "Epoch 17: train_loss=0.5677, val_loss=0.6385, val_acc=0.9111\n",
      "18    0.5675      0.6283      0.9111      \n",
      "Epoch 18: train_loss=0.5675, val_loss=0.6283, val_acc=0.9111\n",
      "19    0.5710      0.6383      0.9111      \n",
      "Epoch 19: train_loss=0.5710, val_loss=0.6383, val_acc=0.9111\n",
      "20    0.5655      0.6257      0.9111      \n",
      "Epoch 20: train_loss=0.5655, val_loss=0.6257, val_acc=0.9111\n",
      "21    0.5630      0.6246      0.9333      \n",
      "Epoch 21: train_loss=0.5630, val_loss=0.6246, val_acc=0.9333\n",
      "22    0.5628      0.6333      0.9111      \n",
      "Epoch 22: train_loss=0.5628, val_loss=0.6333, val_acc=0.9111\n",
      "23    0.5608      0.6335      0.8889      \n",
      "Epoch 23: train_loss=0.5608, val_loss=0.6335, val_acc=0.8889\n",
      "24    0.5633      0.6279      0.9111      \n",
      "Epoch 24: train_loss=0.5633, val_loss=0.6279, val_acc=0.9111\n",
      "25    0.5654      0.6437      0.9111      \n",
      "Epoch 25: train_loss=0.5654, val_loss=0.6437, val_acc=0.9111\n",
      "26    0.5589      0.6344      0.8889      \n",
      "Epoch 26: train_loss=0.5589, val_loss=0.6344, val_acc=0.8889\n",
      "27    0.5596      0.6301      0.9111      \n",
      "Epoch 27: train_loss=0.5596, val_loss=0.6301, val_acc=0.9111\n",
      "28    0.5588      0.6376      0.9111      \n",
      "Epoch 28: train_loss=0.5588, val_loss=0.6376, val_acc=0.9111\n",
      "29    0.5594      0.6356      0.9111      \n",
      "Epoch 29: train_loss=0.5594, val_loss=0.6356, val_acc=0.9111\n",
      "30    0.5577      0.6313      0.9111      \n",
      "Epoch 30: train_loss=0.5577, val_loss=0.6313, val_acc=0.9111\n",
      "31    0.5572      0.6349      0.9111      \n",
      "Epoch 31: train_loss=0.5572, val_loss=0.6349, val_acc=0.9111\n",
      "32    0.5573      0.6323      0.9111      \n",
      "Epoch 32: train_loss=0.5573, val_loss=0.6323, val_acc=0.9111\n",
      "33    0.5570      0.6322      0.9111      \n",
      "Epoch 33: train_loss=0.5570, val_loss=0.6322, val_acc=0.9111\n",
      "34    0.5566      0.6310      0.9111      \n",
      "Epoch 34: train_loss=0.5566, val_loss=0.6310, val_acc=0.9111\n",
      "35    0.5560      0.6322      0.9111      \n",
      "Epoch 35: train_loss=0.5560, val_loss=0.6322, val_acc=0.9111\n",
      "36    0.5595      0.6344      0.9111      \n",
      "Epoch 36: train_loss=0.5595, val_loss=0.6344, val_acc=0.9111\n",
      "37    0.5622      0.6246      0.9333      \n",
      "Epoch 37: train_loss=0.5622, val_loss=0.6246, val_acc=0.9333\n",
      "38    0.5554      0.6438      0.9111      \n",
      "Epoch 38: train_loss=0.5554, val_loss=0.6438, val_acc=0.9111\n",
      "39    0.5593      0.6429      0.9111      \n",
      "Epoch 39: train_loss=0.5593, val_loss=0.6429, val_acc=0.9111\n",
      "40    0.5558      0.6332      0.9333      \n",
      "Epoch 40: train_loss=0.5558, val_loss=0.6332, val_acc=0.9333\n",
      "41    0.5558      0.6288      0.9111      \n",
      "Epoch 41: train_loss=0.5558, val_loss=0.6288, val_acc=0.9111\n",
      "42    0.5553      0.6308      0.9333      \n",
      "Epoch 42: train_loss=0.5553, val_loss=0.6308, val_acc=0.9333\n",
      "43    0.5547      0.6262      0.9111      \n",
      "Epoch 43: train_loss=0.5547, val_loss=0.6262, val_acc=0.9111\n",
      "44    0.5556      0.6264      0.9111      \n",
      "Epoch 44: train_loss=0.5556, val_loss=0.6264, val_acc=0.9111\n",
      "45    0.5564      0.6353      0.9111      \n",
      "Epoch 45: train_loss=0.5564, val_loss=0.6353, val_acc=0.9111\n",
      "46    0.5549      0.6264      0.9111      \n",
      "Epoch 46: train_loss=0.5549, val_loss=0.6264, val_acc=0.9111\n",
      "47    0.5562      0.6269      0.9111      \n",
      "Epoch 47: train_loss=0.5562, val_loss=0.6269, val_acc=0.9111\n",
      "48    0.5538      0.6401      0.9111      \n",
      "Epoch 48: train_loss=0.5538, val_loss=0.6401, val_acc=0.9111\n",
      "49    0.5613      0.6395      0.9111      \n",
      "Epoch 49: train_loss=0.5613, val_loss=0.6395, val_acc=0.9111\n",
      "50    0.5559      0.6223      0.9333      \n",
      "Epoch 50: train_loss=0.5559, val_loss=0.6223, val_acc=0.9333\n",
      "Training completed!\n",
      "Final validation metrics: {'loss': np.float64(0.6222946536996398), 'accuracy': np.float64(0.9333333333333333)}\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "iris_data = load_dataset(\"iris\", batch_size=16, validation_split=0.3)\n",
    "\n",
    "# Create classification model\n",
    "classifier = Sequential(\n",
    "    Linear(4, 16), ReLU(), Linear(16, 8), ReLU(), Linear(8, 3), Softmax()\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SimpleTrainer(\n",
    "    model=classifier,\n",
    "    data=iris_data['train'],\n",
    "    validation_data=iris_data['val'], \n",
    "    loss=\"crossentropy\", \n",
    "    optimizer=\"adam\", \n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Training Iris classifier...\")\n",
    "history = trainer.fit(\n",
    "    epochs=50, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "final_metrics = evaluate(classifier, iris_data[\"val\"], CrossEntropyLoss())\n",
    "print(f\"Final validation metrics: {final_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop for regression...\n",
      "Epoch 0: Loss = 13.2949\n",
      "Epoch 20: Loss = 4.1434\n",
      "Epoch 40: Loss = 0.9452\n",
      "Epoch 60: Loss = 0.4565\n",
      "Epoch 80: Loss = 0.2559\n",
      "Training completed!\n",
      "Test prediction: 3.706, Expected: 3.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1677/1613204889.py:42: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"Test prediction: {float(prediction.data[0]):.3f}, Expected: {expected:.3f}\")\n"
     ]
    }
   ],
   "source": [
    "from fit.optim.adam import Adam\n",
    "from fit.loss.regression import MSELoss\n",
    "\n",
    "# Generate synthetic regression data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(100, 3)\n",
    "true_weights = np.array([1.5, -2.0, 0.5])\n",
    "y_train = X_train @ true_weights + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Create model\n",
    "regression_model = Sequential(Linear(3, 8), ReLU(), Linear(8, 1))\n",
    "\n",
    "# Setup training\n",
    "optimizer = Adam(regression_model.parameters(), lr=0.01)\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "# Custom training loop\n",
    "print(\"Training loop for regression...\")\n",
    "for epoch in range(100):\n",
    "    # Convert to tensors\n",
    "    X_tensor = Tensor(X_train, requires_grad=True)\n",
    "    y_tensor = Tensor(y_train.reshape(-1, 1))\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = regression_model(X_tensor)\n",
    "    loss = loss_fn(predictions, y_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {float(loss.data):.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Test prediction\n",
    "test_X = Tensor([[1.0, -1.0, 0.5]])\n",
    "prediction = regression_model(test_X)\n",
    "expected = 1.0 * 1.5 + (-1.0) * (-2.0) + 0.5 * 0.5  # Using true weights\n",
    "print(f\"Test prediction: {float(prediction.data[0]):.3f}, Expected: {expected:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
