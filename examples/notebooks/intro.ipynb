{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT Library Showcase\n",
    "\n",
    "This notebook demonstrates the features of the FIT library - a machine learning framework built with the help of NumPy.\n",
    "\n",
    "The FIT library that I've created provides tensor operations with automatic differentiation, neural network components with inclusion of linear layers, activations, and normalization, attention mechanisms and transformers, multiple optimizers such as SGD, Adam, SAM, and Lion, loss functions for regression and classification, data pipeline utilities with built-in datasets, training monitoring and visualization, model persistence for saving and loading, and APIs for quick experimentation.\n",
    "\n",
    "I tried to make this library lightweight and it only requires NumPy. It offers a PyTorch-like familiar API, and has an extensible clean architecture for adding components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/Klus3kk/fit.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: Tensor([1. 2. 3.], requires_grad=True)\n",
      "Matrix: Tensor([[1. 2.]\n",
      " [3. 4.]], requires_grad=True)\n",
      "Random tensor: Tensor([[ 0.25159698  0.36753971  1.09971921]\n",
      " [ 0.4132483   1.51174617 -0.2696245 ]\n",
      " [-0.92404499  0.40118634  1.46629455]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Create tensors from various data types\n",
    "a = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "b = Tensor([[1, 2], [3, 4]], requires_grad=True)\n",
    "c = Tensor(np.random.randn(3, 3), requires_grad=True)\n",
    "\n",
    "print(f\"Vector: {a}\")\n",
    "print(f\"Matrix: {b}\")\n",
    "print(f\"Random tensor: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y = [5. 7. 9.]\n",
      "x * y = [ 4. 10. 18.]\n",
      "Matrix multiplication result:\n",
      "[[10.  7.]\n",
      " [22. 15.]]\n"
     ]
    }
   ],
   "source": [
    "# Basic arithmetic\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = Tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "\n",
    "# Addition and multiplication\n",
    "z = x + y\n",
    "w = x * y\n",
    "\n",
    "print(f\"x + y = {z.data}\")\n",
    "print(f\"x * y = {w.data}\")\n",
    "\n",
    "# Matrix operations\n",
    "A = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "B = Tensor([[2.0, 1.0], [4.0, 3.0]], requires_grad=True)\n",
    "C = A @ B  # Matrix multiplication\n",
    "\n",
    "print(f\"Matrix multiplication result:\\n{C.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations & Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic DIfferentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0) = 9.0\n",
      "f'(2.0) = 6.0\n",
      "Expected derivative: 6.0\n"
     ]
    }
   ],
   "source": [
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Function: f(x) = xÂ² + 2x + 1\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x * x + 2 * x + 1\n",
    "print(f\"f({x.data[0]}) = {y.data[0]}\")\n",
    "\n",
    "# Backward pass - compute gradient\n",
    "y.backward()\n",
    "print(f\"f'({x.data[0]}) = {x.grad[0]}\")\n",
    "print(f\"Expected derivative: {2 * x.data[0] + 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a = [3. 4.]\n",
      "Gradient of b = [1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Function: f(a, b) = sum(a * b)\n",
    "a = Tensor([1.0, 2.0], requires_grad=True)\n",
    "b = Tensor([3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "c = a * b\n",
    "loss = c.sum()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Gradient of a = {a.grad}\")\n",
    "print(f\"Gradient of b = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain rule gradient: 20.0\n"
     ]
    }
   ],
   "source": [
    "# Complex function composition\n",
    "x = Tensor([1.0], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y + 3\n",
    "w = z * z\n",
    "\n",
    "w.backward()\n",
    "print(f\"Chain rule gradient: {x.grad[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 3)\n",
      "Output shape: (1, 2)\n",
      "Layer weights shape: (2, 3)\n",
      "Layer bias shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.linear import Linear\n",
    "from fit.core.tensor import Tensor\n",
    "\n",
    "# Create a linear layer\n",
    "layer = Linear(3, 2)  # 3 inputs -> 2 outputs\n",
    "\n",
    "# Forward pass\n",
    "x = Tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "output = layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.data.shape}\")\n",
    "print(f\"Output shape: {output.data.shape}\")\n",
    "print(f\"Layer weights shape: {layer.weight.data.shape}\")\n",
    "print(f\"Layer bias shape: {layer.bias.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[-2. -1.  0.  1.  2.]]\n",
      "ReLU: [[-0. -0.  0.  1.  2.]]\n",
      "Softmax: [[0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]]\n",
      "GELU: [[-0.04540231 -0.15880801  0.          0.84119199  1.95459769]]\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.activation import ReLU, Softmax, GELU\n",
    "\n",
    "# Test different activations\n",
    "x = Tensor([[-2.0, -1.0, 0.0, 1.0, 2.0]], requires_grad=True)\n",
    "\n",
    "relu = ReLU()\n",
    "softmax = Softmax()\n",
    "gelu = GELU()\n",
    "\n",
    "print(f\"Input: {x.data}\")\n",
    "print(f\"ReLU: {relu(x).data}\")\n",
    "print(f\"Softmax: {softmax(x).data}\")\n",
    "print(f\"GELU: {gelu(x).data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [[0.84966947 0.07866769 0.07166284]]\n",
      "Output sum (should be ~1.0): 1.0\n",
      "Number of parameters: 6\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.container import Sequential\n",
    "from fit.nn.modules.linear import Linear\n",
    "from fit.nn.modules.activation import ReLU, Softmax\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential(\n",
    "    Linear(4, 8),  # Input layer\n",
    "    ReLU(),  # Activation\n",
    "    Linear(8, 6),  # Hidden layer\n",
    "    ReLU(),  # Activation\n",
    "    Linear(6, 3),  # Output layer\n",
    "    Softmax(),  # Final activation\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "x = Tensor([[1.0, 2.0, 3.0, 4.0]], requires_grad=True)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Model output: {output.data}\")\n",
    "print(f\"Output sum (should be ~1.0): {output.data.sum()}\")\n",
    "print(f\"Number of parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [[1. 2. 3. 4.]\n",
      " [2. 3. 4. 5.]]\n",
      "Batch normalized: [[-0.99998 -0.99998 -0.99998 -0.99998]\n",
      " [ 0.99998  0.99998  0.99998  0.99998]]\n",
      "Layer normalized: [[-1.34163542 -0.44721181  0.44721181  1.34163542]\n",
      " [-1.34163542 -0.44721181  0.44721181  1.34163542]]\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.normalization import BatchNorm, LayerNorm\n",
    "\n",
    "# Batch normalization\n",
    "batch_norm = BatchNorm(4)\n",
    "x = Tensor([[1.0, 2.0, 3.0, 4.0], [2.0, 3.0, 4.0, 5.0]], requires_grad=True)\n",
    "normalized = batch_norm(x)\n",
    "\n",
    "print(f\"Original: {x.data}\")\n",
    "print(f\"Batch normalized: {normalized.data}\")\n",
    "\n",
    "# Layer normalization\n",
    "layer_norm = LayerNorm(4)\n",
    "layer_normalized = layer_norm(x)\n",
    "print(f\"Layer normalized: {layer_normalized.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.794796120705038\n",
      "Training step completed!\n"
     ]
    }
   ],
   "source": [
    "from fit.optim.sgd import SGD\n",
    "from fit.nn.modules.linear import Linear\n",
    "\n",
    "# Create model and optimizer\n",
    "model = Linear(2, 1)\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Simulate training step\n",
    "x = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "target = Tensor([[3.0]])\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "loss = ((output - target) ** 2).mean()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Optimizer step\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(f\"Loss: {loss.data}\")\n",
    "print(\"Training step completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: 0.2421\n",
      "Step 2, Loss: 2.3305\n",
      "Step 3, Loss: 0.3512\n",
      "Step 4, Loss: 0.2987\n",
      "Step 5, Loss: 0.0059\n"
     ]
    }
   ],
   "source": [
    "from fit.optim.adam import Adam\n",
    "\n",
    "# Create model and Adam optimizer\n",
    "model = Sequential(Linear(2, 4), ReLU(), Linear(4, 1))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# Training loop simulation\n",
    "for step in range(5):\n",
    "    # Sample data\n",
    "    x = Tensor([[np.random.randn(), np.random.randn()]], requires_grad=True)\n",
    "    target = Tensor([[np.random.randn()]])\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    loss = ((output - target) ** 2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Fix: Handle both scalar and array loss values\n",
    "    if loss.data.ndim == 0:\n",
    "        # Scalar value\n",
    "        loss_value = float(loss.data)\n",
    "    else:\n",
    "        # Array value\n",
    "        loss_value = float(loss.data[0])\n",
    "\n",
    "    print(f\"Step {step+1}, Loss: {loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100\n",
      "Number of batches: 7\n",
      "Batch 1: X shape (16, 4), y shape (16,)\n",
      "Batch 2: X shape (16, 4), y shape (16,)\n",
      "Batch 3: X shape (16, 4), y shape (16,)\n"
     ]
    }
   ],
   "source": [
    "from fit.data.dataset import Dataset\n",
    "from fit.data.dataloader import DataLoader\n",
    "\n",
    "# Create custom dataset\n",
    "X = np.random.randn(100, 4)\n",
    "y = np.random.randint(0, 3, 100)\n",
    "\n",
    "dataset = Dataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Iterate through batches\n",
    "for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "    print(f\"Batch {i+1}: X shape {batch_x.data.shape}, y shape {batch_y.data.shape}\")\n",
    "    if i >= 2:  # Show first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating XOR dataset...\n",
      "XOR Dataset loaded:\n",
      "Train loader batches: 250\n",
      "Loading Iris dataset...\n",
      "Iris Dataset loaded:\n",
      "Train batches: 4\n",
      "Validation batches: 1\n",
      "XOR - X: [[1. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]], y: [0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from fit.simple.data import load_dataset\n",
    "\n",
    "# Load XOR dataset\n",
    "xor_data = load_dataset(\"xor\", batch_size=4)\n",
    "print(\"XOR Dataset loaded:\")\n",
    "print(f\"Train loader batches: {len(xor_data['train'])}\")\n",
    "\n",
    "# Load Iris dataset\n",
    "iris_data = load_dataset(\"iris\", batch_size=32, validation_split=0.2)\n",
    "print(\"Iris Dataset loaded:\")\n",
    "print(f\"Train batches: {len(iris_data['train'])}\")\n",
    "print(f\"Validation batches: {len(iris_data['val'])}\")\n",
    "\n",
    "# Sample from XOR dataset\n",
    "for x, y in xor_data[\"train\"]:\n",
    "    print(f\"XOR - X: {x.data}, y: {y.data}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 10\n",
      "Selected features: 5\n",
      "Selected feature indices: [ True  True False False  True False  True False False  True]\n"
     ]
    }
   ],
   "source": [
    "from fit.data.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.randn(100, 10)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "print(f\"Selected feature indices: {selector.get_support()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.024999999999999977\n",
      "Gradients: [[-0.1   0.1 ]\n",
      " [ 0.05 -0.05]]\n"
     ]
    }
   ],
   "source": [
    "from fit.loss.regression import MSELoss\n",
    "\n",
    "mse = MSELoss()\n",
    "\n",
    "# Sample predictions and targets\n",
    "predictions = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "targets = Tensor([[1.2, 1.8], [2.9, 4.1]])\n",
    "\n",
    "loss = mse(predictions, targets)\n",
    "print(f\"MSE Loss: {loss.data}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(f\"Gradients: {predictions.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy Loss: 0.419290646726997\n",
      "Logits gradients: [[-0.17049943  0.12121649  0.04928295]\n",
      " [ 0.07318986 -0.17198583  0.09879597]]\n"
     ]
    }
   ],
   "source": [
    "from fit.loss.classification import CrossEntropyLoss\n",
    "\n",
    "ce_loss = CrossEntropyLoss()\n",
    "\n",
    "# Logits and class labels\n",
    "logits = Tensor([[2.0, 1.0, 0.1], [0.5, 2.0, 0.8]], requires_grad=True)\n",
    "targets = Tensor([0, 1])  # Class indices\n",
    "\n",
    "loss = ce_loss(logits, targets)\n",
    "print(f\"CrossEntropy Loss: {loss.data}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(f\"Logits gradients: {logits.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention & Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention input shape: (2, 10, 64)\n",
      "Attention output shape: (2, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.attention import MultiHeadAttention\n",
    "\n",
    "# Create multi-head attention\n",
    "attention = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "\n",
    "# Sample input (batch_size=2, seq_len=10, d_model=64)\n",
    "x = Tensor(np.random.randn(2, 10, 64), requires_grad=True)\n",
    "\n",
    "# Self-attention\n",
    "output = attention(x, x, x)\n",
    "print(f\"Attention input shape: {x.data.shape}\")\n",
    "print(f\"Attention output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer input shape: (2, 10, 64)\n",
      "Transformer output shape: (2, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.modules.transformer import TransformerEncoderBlock\n",
    "\n",
    "# Create transformer encoder block\n",
    "transformer_block = TransformerEncoderBlock(\n",
    "    d_model=64, num_heads=8, d_ff=256, dropout=0.1\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "x = Tensor(np.random.randn(2, 10, 64), requires_grad=True)\n",
    "output = transformer_block(x)\n",
    "\n",
    "print(f\"Transformer input shape: {x.data.shape}\")\n",
    "print(f\"Transformer output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "from fit.nn.utils.spectral_norm import SpectralNormLinear\n",
    "\n",
    "# Linear layer with spectral normalization\n",
    "spec_linear = SpectralNormLinear(10, 5, n_power_iterations=1)\n",
    "\n",
    "x = Tensor(np.random.randn(3, 10), requires_grad=True)\n",
    "output = spec_linear(x)\n",
    "\n",
    "print(f\"Output shape: {output.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train_loss  val_loss    accuracy    Time    \n",
      "--------------------------------------------------\n",
      "1     0.9928      1.2488      0.0071      \n",
      "2     0.9054      1.1707      0.1051      \n",
      "3     0.8113      1.0706      0.1652      \n",
      "4     0.6538      0.9592      0.2041      \n",
      "5     0.6012      0.9884      0.3324      \n",
      "6     0.5177      0.7813      0.3942      \n",
      "7     0.4032      0.7712      0.4748      \n",
      "8     0.3692      0.5478      0.5609      \n",
      "9     0.1141      0.4863      0.6513      \n",
      "10    0.1353      0.4451      0.7141      \n",
      "Metrics logged!\n",
      "Best validation loss: 0.4450874973037048\n",
      "Logs exported to training_log.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'training_log.json'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fit.monitor.tracker import TrainingTracker\n",
    "\n",
    "# Create tracker\n",
    "tracker = TrainingTracker(experiment_name=\"demo_experiment\")\n",
    "\n",
    "# Simulate training with metrics\n",
    "for epoch in range(10):\n",
    "    # Simulate epoch metrics\n",
    "    train_loss = 1.0 - epoch * 0.1 + np.random.normal(0, 0.05)\n",
    "    val_loss = 1.2 - epoch * 0.08 + np.random.normal(0, 0.08)\n",
    "    accuracy = epoch * 0.08 + np.random.normal(0, 0.02)\n",
    "\n",
    "    # Log metrics\n",
    "    tracker.update(\n",
    "        epoch, {\"train_loss\": train_loss, \"val_loss\": val_loss, \"accuracy\": accuracy}\n",
    "    )\n",
    "\n",
    "    tracker.log_learning_rate(0.001 * (0.9**epoch))\n",
    "\n",
    "print(\"Metrics logged!\")\n",
    "print(f\"Best validation loss: {tracker.best_values.get('val_loss', 'N/A')}\")\n",
    "\n",
    "# Export metrics\n",
    "tracker.export(\"training_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'loss': np.float64(1.1119251456841315), 'accuracy': np.float64(0.35)}\n"
     ]
    }
   ],
   "source": [
    "from fit.utils.engine import evaluate\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "model = Sequential(Linear(4, 8), ReLU(), Linear(8, 3), Softmax())\n",
    "\n",
    "# Create sample data\n",
    "X = Tensor(np.random.randn(20, 4))\n",
    "y = Tensor(np.random.randint(0, 3, 20))\n",
    "dataset = Dataset(X.data, y.data)\n",
    "dataloader = DataLoader(dataset, batch_size=5)\n",
    "\n",
    "# Evaluate model\n",
    "from fit.loss.classification import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "metrics = evaluate(model, dataloader, loss_fn)\n",
    "print(f\"Evaluation metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model save/load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'Tensor.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential(\n\u001b[1;32m      5\u001b[0m     Linear(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m      6\u001b[0m     ReLU(),\n\u001b[1;32m      7\u001b[0m     Linear(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo_model.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Codes/fit/fit/nn/utils/model_io.py:33\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, path)\u001b[0m\n\u001b[1;32m     27\u001b[0m save_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m: model_state\n\u001b[1;32m     30\u001b[0m }\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'Tensor.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "from fit.nn.utils.model_io import save_model, load_model\n",
    "\n",
    "# Create and train a simple model\n",
    "model = Sequential(Linear(2, 4), ReLU(), Linear(4, 1))\n",
    "\n",
    "# Save model\n",
    "save_model(model, \"demo_model.pkl\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = load_model(\"demo_model.pkl\")\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Test that loaded model works\n",
    "test_input = Tensor([[1.0, 2.0]])\n",
    "original_output = model(test_input)\n",
    "loaded_output = loaded_model(test_input)\n",
    "\n",
    "print(f\"Original output: {original_output.data}\")\n",
    "print(f\"Loaded output: {loaded_output.data}\")\n",
    "print(f\"Outputs match: {np.allclose(original_output.data, loaded_output.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.simple.trainer import Trainer\n",
    "\n",
    "# Create XOR model\n",
    "xor_model = Sequential(Linear(2, 8), ReLU(), Linear(8, 4), ReLU(), Linear(4, 1))\n",
    "\n",
    "# XOR data\n",
    "X = Tensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = Tensor([[0], [1], [1], [0]])\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(model=xor_model, loss=\"mse\", optimizer=\"adam\", lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training XOR model...\")\n",
    "history = trainer.fit(\n",
    "    data=(X, y),\n",
    "    epochs=100,\n",
    "    batch_size=4,\n",
    "    validation_split=0.0,  # Use all data for training\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Test the trained model\n",
    "print(\"\\nXOR Results:\")\n",
    "for i, (input_val, expected) in enumerate(zip(X.data, y.data)):\n",
    "    prediction = xor_model(Tensor([input_val]))\n",
    "    print(\n",
    "        f\"Input: {input_val}, Expected: {expected[0]:.0f}, Predicted: {prediction.data[0]:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris_data = load_dataset(\"iris\", batch_size=16, validation_split=0.3)\n",
    "\n",
    "# Create classification model\n",
    "classifier = Sequential(\n",
    "    Linear(4, 16), ReLU(), Linear(16, 8), ReLU(), Linear(8, 3), Softmax()\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(model=classifier, loss=\"crossentropy\", optimizer=\"adam\", lr=0.01)\n",
    "\n",
    "# Train\n",
    "print(\"Training Iris classifier...\")\n",
    "history = trainer.fit(\n",
    "    data=iris_data[\"train\"], validation_data=iris_data[\"val\"], epochs=50, verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "final_metrics = evaluate(classifier, iris_data[\"val\"], CrossEntropyLoss())\n",
    "print(f\"Final validation metrics: {final_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit.optim.adam import Adam\n",
    "from fit.loss.regression import MSELoss\n",
    "\n",
    "# Generate synthetic regression data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(100, 3)\n",
    "true_weights = np.array([1.5, -2.0, 0.5])\n",
    "y_train = X_train @ true_weights + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Create model\n",
    "regression_model = Sequential(Linear(3, 8), ReLU(), Linear(8, 1))\n",
    "\n",
    "# Setup training\n",
    "optimizer = Adam(regression_model.parameters(), lr=0.01)\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "# Custom training loop\n",
    "print(\"Training loop for regression...\")\n",
    "for epoch in range(100):\n",
    "    # Convert to tensors\n",
    "    X_tensor = Tensor(X_train, requires_grad=True)\n",
    "    y_tensor = Tensor(y_train.reshape(-1, 1))\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = regression_model(X_tensor)\n",
    "    loss = loss_fn(predictions, y_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.data[0]:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Test prediction\n",
    "test_X = Tensor([[1.0, -1.0, 0.5]])\n",
    "prediction = regression_model(test_X)\n",
    "expected = 1.0 * 1.5 + (-1.0) * (-2.0) + 0.5 * 0.5  # Using true weights\n",
    "print(f\"Test prediction: {prediction.data[0]:.3f}, Expected: {expected:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
